{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a03297f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rishi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/rishi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /Users/rishi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.initializers import Constant\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from keras.layers import Dense,SpatialDropout1D\n",
    "import contractions\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# initializing Stop words libraries\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.tokenize import word_tokenize\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f6ba97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12930 entries, 0 to 12929\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   job_title         12930 non-null  object \n",
      " 1   cleaned_keywords  12930 non-null  object \n",
      " 2   total_score       12930 non-null  float64\n",
      " 3   Total_jobs        12930 non-null  int64  \n",
      " 4   avg_score         12930 non-null  float64\n",
      "dtypes: float64(2), int64(1), object(2)\n",
      "memory usage: 505.2+ KB\n"
     ]
    }
   ],
   "source": [
    "#results for job data - keywords and their scores\n",
    "result_df2 = pd.read_excel(\"job_lvl_keywords.xlsx\")\n",
    "result_df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aace90d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data processing and key word extraction functions - takes the resume content as input string\n",
    "def tokenize_and_tag(desc):\n",
    "    tokens = nltk.word_tokenize(desc.lower())\n",
    "    filtered_tokens = [w for w in tokens if not w in stop_words]\n",
    "    tagged = nltk.pos_tag(filtered_tokens)\n",
    "    return tagged\n",
    "\n",
    "def extract_POS(tagged):\n",
    "    #pattern 1\n",
    "    grammar1 = ('''Noun Phrases: {<DT>?<JJ>*<NN|NNS|NNP>+}''')\n",
    "    chunkParser = nltk.RegexpParser(grammar1)\n",
    "    tree1 = chunkParser.parse(tagged)\n",
    "\n",
    "    # typical noun phrase pattern appending to be concatted later\n",
    "    g1_chunks = []\n",
    "    for subtree in tree1.subtrees(filter=lambda t: t.label() == 'Noun Phrases'):\n",
    "        g1_chunks.append(subtree)\n",
    "\n",
    "    #pattern 2\n",
    "    grammar2 = ('''NP2: {<IN>?<JJ|NN>*<NNS|NN>} ''')\n",
    "    chunkParser = nltk.RegexpParser(grammar2)\n",
    "    tree2 = chunkParser.parse(tagged)\n",
    "\n",
    "    # variation of a noun phrase pattern to be pickled for later analyses\n",
    "    g2_chunks = []\n",
    "    for subtree in tree2.subtrees(filter=lambda t: t.label() == 'NP2'):\n",
    "        g2_chunks.append(subtree)\n",
    "\n",
    "    #pattern 3\n",
    "    grammar3 = (''' VS: {<VBG|VBZ|VBP|VBD|VB|VBN><NNS|NN>*}''')\n",
    "    chunkParser = nltk.RegexpParser(grammar3)\n",
    "    tree3 = chunkParser.parse(tagged)\n",
    "\n",
    "    # verb-noun pattern appending to be concatted later\n",
    "    g3_chunks = []\n",
    "    for subtree in tree3.subtrees(filter=lambda t: t.label() == 'VS'):\n",
    "        g3_chunks.append(subtree)\n",
    "\n",
    "\n",
    "    # pattern 4\n",
    "    # any number of a singular or plural noun followed by a comma followed by the same noun, noun, noun pattern\n",
    "    grammar4 = ('''Commas: {<NN|NNS>*<,><NN|NNS>*<,><NN|NNS>*} ''')\n",
    "    chunkParser = nltk.RegexpParser(grammar4)\n",
    "    tree4 = chunkParser.parse(tagged)\n",
    "\n",
    "    # common pattern of listing skills appending to be concatted later\n",
    "    g4_chunks = []\n",
    "    for subtree in tree4.subtrees(filter=lambda t: t.label() == 'Commas'):\n",
    "        g4_chunks.append(subtree)\n",
    "\n",
    "    return g1_chunks, g2_chunks, g3_chunks, g4_chunks\n",
    "\n",
    "def training_set(chunks):\n",
    "    '''creates a dataframe that easily parsed with the chunks data '''\n",
    "    df = pd.DataFrame(chunks)\n",
    "    df.fillna('X', inplace = True)\n",
    "\n",
    "    train = []\n",
    "    for row in df.values:\n",
    "        phrase = ''\n",
    "        for tup in row:\n",
    "            # needs a space at the end for seperation\n",
    "            phrase += tup[0] + ' '\n",
    "        phrase = ''.join(phrase)\n",
    "        # could use padding tages but encoder method will provide during\n",
    "        # tokenizing/embeddings; X can replace paddding for now\n",
    "        train.append( phrase.replace('X', '').strip())\n",
    "\n",
    "    df['phrase'] = train\n",
    "\n",
    "    #returns 50% of each dataframe to be used if you want to improve execution time\n",
    "    # return df.phrase.sample(frac = 0.5)\n",
    "    # Update: only do 50% if running on excel\n",
    "    return df.phrase\n",
    "\n",
    "def strip_commas(df):\n",
    "    '''create new series of individual n-grams'''\n",
    "    grams = []\n",
    "    for sen in df:\n",
    "        sent = sen.split(',')\n",
    "        for word in sent:\n",
    "            grams.append(word)\n",
    "    return pd.Series(grams)\n",
    "\n",
    "def generate_phrases(desc):\n",
    "    tagged = tokenize_and_tag(desc)\n",
    "    g1_chunks, g2_chunks, g3_chunks, g4_chunks = extract_POS(tagged)\n",
    "    c = training_set(g4_chunks)\n",
    "    separated_chunks4 = strip_commas(c)\n",
    "    phrases = pd.concat([training_set(g1_chunks),\n",
    "                          training_set(g2_chunks),\n",
    "                          training_set(g3_chunks),\n",
    "                          separated_chunks4],\n",
    "                            ignore_index = True )\n",
    "    return phrases\n",
    "\n",
    "\"\"\"Creates corpus from feature column, which is a pandas series\"\"\"\n",
    "def create_corpus(df):\n",
    "    corpus=[]\n",
    "    for phrase in tqdm(df):\n",
    "        words=[word.lower() for word in word_tokenize(phrase) if(word.isalpha()==1)]\n",
    "        corpus.append(words)\n",
    "    return corpus\n",
    "\n",
    "\"\"\"Create padded sequences of equal lenght as input to LSTM\"\"\"\n",
    "def create_padded_inputs(corpus):\n",
    "    MAX_LEN=20\n",
    "    tokenizer_obj=Tokenizer()\n",
    "    tokenizer_obj.fit_on_texts(corpus)\n",
    "    sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
    "\n",
    "    phrase_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')\n",
    "    return phrase_pad\n",
    "\n",
    "def clean(desc):\n",
    "    desc = contractions.fix(desc)\n",
    "    desc = re.sub(\"[!@.$\\'\\'':()]\", \"\", desc)\n",
    "    return desc\n",
    "\n",
    "def get_predictions(desc):\n",
    "    #clean\n",
    "    desc = clean(desc)\n",
    "    #load model\n",
    "    model = tf.keras.models.load_model('/Users/rishi/Documents/CSE 6242/group project/main/lstm_skill_extractor.h5')\n",
    "    #tokenize and convert to phrases\n",
    "    phrases = generate_phrases(desc)\n",
    "    #preprocess unseen data\n",
    "    corpus=create_corpus(phrases)\n",
    "    corpus_pad = create_padded_inputs(corpus)\n",
    "    #get predicted classes\n",
    "    predictions = (model.predict(corpus_pad) >0.4).astype('int32')\n",
    "    #return predicted skills as list\n",
    "    out = pd.DataFrame({'Phrase':phrases, 'Class':predictions.ravel(), 'Scores': model.predict(corpus_pad).ravel()})\n",
    "    skills = out.loc[out['Class'] == 1].sort_values(by='Scores',ascending=False)\n",
    "    return  skills[['Phrase','Scores']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "802a1f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this input would actually come from an API request based on user input\n",
    "#this is a placeholder as we currently dont have the feature implemented\n",
    "#generated from keyword_extraction_model-main.ipynb\n",
    "resume_df = pd.read_csv(\"resume_dataset.csv\", delimiter=\",\", encoding='utf-8')\n",
    "resume = resume_df['Resume'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b77de091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 535/535 [00:00<00:00, 50566.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 2ms/step\n",
      "17/17 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deep learningeducation details data science as...</td>\n",
       "      <td>0.865631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fraud analytic platform fraud analytics</td>\n",
       "      <td>0.837631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>customer feedback survey data</td>\n",
       "      <td>0.833121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>received customer feedback survey data</td>\n",
       "      <td>0.829445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>multiple data science</td>\n",
       "      <td>0.818370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Phrase    Scores\n",
       "0  deep learningeducation details data science as...  0.865631\n",
       "1            fraud analytic platform fraud analytics  0.837631\n",
       "2                      customer feedback survey data  0.833121\n",
       "3             received customer feedback survey data  0.829445\n",
       "4                              multiple data science  0.818370"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calls the above functions and generates a df with key phrases and their scores\n",
    "# we need to further extract keywords from the phrases.\n",
    "columns = ['Phrase', 'Scores']\n",
    "resume_keywords = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "    #if(i>100):\n",
    "        #break\n",
    "    #print(type(r['Translated_Desc']))\n",
    "t = get_predictions(resume)\n",
    "    #print(t)\n",
    "t['Phrase'] = t['Phrase'].apply(lambda x: x.strip())\n",
    "t.drop_duplicates(subset='Phrase',inplace=True)\n",
    "resume_keywords = pd.concat([resume_keywords, t], ignore_index=True)\n",
    "#resume_keywords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7939da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to account for any issues with spaces between words after keyword extraction - \n",
    "#we added a list of common skills for sub string matching.\n",
    "# eg 'pythonjavac...' the model wont recognize them as seperate words.\n",
    "#This was mainly an issue with webscraped data with parsing issues.\n",
    "skills_list = [\n",
    "    'IDE','CMS','CRM','ERP','VCS','CI/CD','API','SDK','CMS',\n",
    "    'DBMS',\n",
    "    'RDBMS',\n",
    "    'NoSQL',\n",
    "    'SQL',\n",
    "    'HTTP',\n",
    "    'HTTPS',\n",
    "    'FTP',\n",
    "    'SSH',\n",
    "    'TCPIP',\n",
    "    'DNS',\n",
    "    'VPN',\n",
    "    'IoT',\n",
    "    'JSON',\n",
    "    'XML',\n",
    "    'REST',\n",
    "    'SOAP',\n",
    "    'GraphQL',\n",
    "    'SaaS',\n",
    "    'PaaS',\n",
    "    'IaaS',\n",
    "    'DaaS',\n",
    "    'MLaaS',\n",
    "    'NLP',\n",
    "    'VR',\n",
    "    'Augmentedreality',\n",
    "    'UI/UX',\n",
    "    'UX','uxui','uiux',\n",
    "    'API Gateway',\n",
    "    'LoadBalancer',\n",
    "    'Firewall',\n",
    "    'ReverseProxy',\n",
    "    'Containerization',\n",
    "    'Orchestration',\n",
    "    'ServerlessComputing',\n",
    "    'MicroservicesArchitecture',\n",
    "    'CDN',\n",
    "    'FaaS',\n",
    "    'CIAM',\n",
    "    'SIEM',\n",
    "    'EDM',\n",
    "    'EDA',\n",
    "    'CICD',\n",
    "    'SSO',\n",
    "    'JWT',\n",
    "    'OAuth',\n",
    "    'SSL',\n",
    "    'TLS',\n",
    "    'Docker',\n",
    "    'Kubernetes',\n",
    "    'Ansible',\n",
    "    'Jenkins',\n",
    "    'Git',\n",
    "    'GitHub',\n",
    "    'GitLab',\n",
    "    'Bitbucket',\n",
    "    'Jira',\n",
    "    'Trello',\n",
    "    'Confluence',\n",
    "    'Slack',\n",
    "    'Zoom',\n",
    "    'MicrosoftTeams',\n",
    "    'GoogleWorkspace',\n",
    "    'Office 365',\n",
    "    'AWS',\n",
    "    'Azure',\n",
    "    'GCP',\n",
    "    'Cloud',\n",
    "    'Heroku',\n",
    "    'Firebase',\n",
    "    'Netlify',\n",
    "    'Vercel',\n",
    "    'NetBeans',\n",
    "    'Eclipse',\n",
    "    'Visual Studio',\n",
    "    'SublimeText',\n",
    "    'Atom',\n",
    "    'PyCharm',\n",
    "    'IntelliJ', 'IDEA',\n",
    "    'VS',\n",
    "    'Postman',\n",
    "    'Swagger',\n",
    "    'Insomnia',\n",
    "    'Wireshark',\n",
    "    'PostgreSQL',\n",
    "    'MySQL',\n",
    "    'SQLite',\n",
    "    'MongoDB',\n",
    "    'Cassandra',\n",
    "    'Redis',\n",
    "    'Elasticsearch',\n",
    "    'Oracle',\n",
    "    'Firebase', 'Firestore',\n",
    "    'Neo4j',\n",
    "    'RabbitMQ',\n",
    "    'Kafka',\n",
    "    'Hadoop',\n",
    "    'Spark',\n",
    "    'TensorFlow',\n",
    "    'PyTorch',\n",
    "    'Scikitlearn',\n",
    "    'Pandas',\n",
    "    'NumPy',\n",
    "    'Matplotlib',\n",
    "    'Seaborn',\n",
    "    'Bokeh',\n",
    "    'Plotly',\n",
    "    'Tableau',\n",
    "    'PowerBI',\n",
    "    'Excel',\n",
    "    'GoogleSheets',\n",
    "    'Airflow',\n",
    "    'Luigi',\n",
    "    'Glue',\n",
    "    'Talend',\n",
    "    'NiFi',\n",
    "    'PowerShell',\n",
    "    'Bash',\n",
    "    'Python',\n",
    "    'JavaScript',\n",
    "    'Java',\n",
    "    'C++',\n",
    "    'C#',\n",
    "    'Ruby',\n",
    "    'Swift',\n",
    "    'Kotlin',\n",
    "    'TypeScript',\n",
    "    'HTML',\n",
    "    'CSS',\n",
    "    'Sass',\n",
    "    'React',\n",
    "    'Angular',\n",
    "    'Vuejs',\n",
    "    'Nodejs',\n",
    "    'Expressjs',\n",
    "    'Django',\n",
    "    'Flask',\n",
    "    'SpringBoot',\n",
    "    'Laravel',\n",
    "    'Symfony',\n",
    "    'Ruby',\n",
    "    'ASP.NET',\n",
    "    'Bootstrap',\n",
    "    'TailwindCSS',\n",
    "    'MaterialUI',\n",
    "    'Ant Design',\n",
    "    'D3js',\n",
    "    'Threejs',\n",
    "    'Unity',\n",
    "    'UnrealEngine',\n",
    "    'AndroidStudio',\n",
    "    'Xcode',\n",
    "    'Flutter',\n",
    "    'react',\n",
    "    'ReactNative',\n",
    "    'Ionic',\n",
    "    'Cordova',\n",
    "    'Xamarin',\n",
    "    'Redux',\n",
    "    'MobX',\n",
    "    'Vuex',\n",
    "    'GraphQL',\n",
    "    'ApolloClient',\n",
    "    'ReduxSaga',\n",
    "    'RxJS',\n",
    "    'Jest',\n",
    "    'Mocha',\n",
    "    'Chai',\n",
    "    'Cypress',\n",
    "    'Selenium',\n",
    "    'JUnit',\n",
    "    'TestNG',\n",
    "    'Appium',\n",
    "    'Detox',\n",
    "    'JUnit',\n",
    "    'TestNG',\n",
    "    'RobotFramework',\n",
    "    'SoapUI',\n",
    "    'Jira',\n",
    "    'TestRail',\n",
    "    'Confluence',\n",
    "    'Zephyr',\n",
    "    'Gatling',\n",
    "    'Locust',\n",
    "    'ApacheJMeter',\n",
    "    'LoadRunner',\n",
    "    'Nessus',\n",
    "    'Wireshark',\n",
    "    'BurpSuite',\n",
    "    'Metasploit',\n",
    "    'Nmap',\n",
    "    'Splunk',\n",
    "    'Logstash',\n",
    "    'Kibana',\n",
    "    'ELKStack',\n",
    "    'ELK',\n",
    "    'QRadar',\n",
    "    'ArcSight',\n",
    "    'AzureSentinel',\n",
    "    'Graylog',\n",
    "    'Loggly',\n",
    "    'Auth0',\n",
    "    'Okta',\n",
    "    'PingIdentity',\n",
    "    'Keycloak',\n",
    "    'Cognito',\n",
    "    'AzureAD',\n",
    "    'OneLogin',\n",
    "    'ForgeRock',\n",
    "    'GoogleCloud',\n",
    "    'JWTio',\n",
    "    'OAuthio',\n",
    "    'SSLMate',\n",
    "    'Digicert',\n",
    "    'LetsEncrypt',\n",
    "    'HashiCorpVault',\n",
    "    'LastPass',\n",
    "    '1Password',\n",
    "    'KeePass',\n",
    "    'Dashlane',\n",
    "    'BitLocker',\n",
    "    'VeraCrypt',\n",
    "    'Norton',\n",
    "    'McAfee',\n",
    "    'Avast',\n",
    "    'Kaspersky',\n",
    "    'Sophos',\n",
    "    'ClamAV',\n",
    "    'Malwarebytes',\n",
    "    'TrendMicro',\n",
    "    'Bitdefender',\n",
    "    'FireEye',\n",
    "    'CrowdStrike',\n",
    "    'Symantec',\n",
    "    'Fortinet',\n",
    "    'Cisco',\n",
    "    'Zscaler',\n",
    "    'Akamai',\n",
    "    'Cloudflare',\n",
    "    'Imperva',\n",
    "    'F5Networks',\n",
    "    'Barracuda',\n",
    "    'DL',\n",
    "    'LLM',\n",
    "    'GPT',\n",
    "    'Analysis',\n",
    "    'patterns',\n",
    "    'visual',\n",
    "    'query',\n",
    "    'rstudio',\n",
    "    'angular',\n",
    "    'algorithm',\n",
    "    'nueralnetworks',\n",
    "    'engineer',\n",
    "    'customer',\n",
    "    'testing',\n",
    "    'model',\n",
    "    'analytic',\n",
    "    'process',\n",
    "    'function',\n",
    "    'consult',\n",
    "    'service',\n",
    "    'project',\n",
    "    'database',\n",
    "    'analyze',\n",
    "    'artificialintelligence',\n",
    "    'kpi',\n",
    "    'nlp',\n",
    "    'naturallanguage',\n",
    "    'sql',\n",
    "    'dbms','data','database','code','scrape',\n",
    "    'pca','nosql','olap','api','sdk','saas','uml','ebitda','manage','design','stakeholder','hadoop','spark',\n",
    "    'pyspark','athena','s3','gcc','sage','mapreduce','regression','classification','randomforest','xgboost','gradient',\n",
    "    'cluster','svm','bayes',\"statistic\",'project','product','scala','visualization','d3','golang','stack','php',\n",
    "    'clean','django','analyst',\"flask\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e49a7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign some generic abbrev in tech for overwriting parts of speech conditions.\n",
    "all_abbreviations = [\n",
    "    # Data Science\n",
    "    \"EDA\", \"ML\", \"AI\", \"DS\", \"NLP\", \"CV\", \"PCA\", \"OLS\", \"ANOVA\", \"ROC\", \"AUC\", \"RMSE\", \"KPI\", \"ETL\", \"BI\", \"SQL\",\"R\",\n",
    "    \"AWS\",'RF',\n",
    "    \n",
    "    # Database\n",
    "    \"DBMS\", \"SQL\", \"NoSQL\", \"RDBMS\", \"DDL\", \"DML\", \"ACID\", \"CAP\", \"OLAP\", \"OLTP\", \"MDM\", \"ETL\",\"SAS\",\n",
    "    \n",
    "    # Software\n",
    "    \"IDE\", \"API\", \"SDK\", \"CLI\", \"GUI\", \"UI\", \"UX\", \"CI/CD\", \"VCS\", \"CMS\", \"ERP\", \"SaaS\", \"IoT\", \"DevOps\",\"uml\",\n",
    "    \n",
    "    # Product Management\n",
    "    \"PM\", \"PO\", \"MVP\", \"KPI\", \"OKR\", \"USP\", \"B2B\", \"B2C\", \"ROI\", \"MRR\", \"CAC\", \"LTV\", \"NPS\",\n",
    "    \n",
    "    # Finance\n",
    "    \"ROI\", \"ROE\", \"EPS\", \"P/E\", \"DCF\", \"IRR\", \"EBITDA\", \"CAGR\", \"AUM\", \"FOMO\", \"ETF\", \"IPO\", \"GDPR\", \"KYC\", \"AML\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d18cb51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cognito'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "key_word_list = list(set([s.lower() for s in skills_list]))\n",
    "key_word_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4880c682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "art\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#adding generic stop words for job descriptions to existing list of stop words - \n",
    "#a draw back withour approach vs TFIDF where the common words are penalized\n",
    "import spacy\n",
    "\n",
    "\n",
    "# Define other stop words\n",
    "other_stop_words = ['junior', 'senior', 'experience', 'etc', 'job', 'work', 'company', 'technique',\n",
    "                    'candidate', 'skill', 'skills', 'menu', 'inc', 'new', 'plus', 'years',\n",
    "                    'technology', 'ceo', 'cto', 'account','good','understanding',\n",
    "                    'strong', 'specification', 'popular', 'essential','required','preferred','requirement',\n",
    "                    'satisfy','people','resume','resumes','opportunities','able','responsibilities',\n",
    "                    'group','distribution','potential','given','nondiscrimination','discrimination',\n",
    "                    'transparency','seniority','ability','world','international','approach','dedicated','global','region','regions'\n",
    "                   'responsibilities', 'qualifications', 'requirements', 'benefits', 'responsibility',\n",
    "                    'qualification', 'requirement', 'benefit', 'role', 'position','specific','looking',\n",
    "                    'opportunity', 'knowledge', 'abilities', 'team', 'collaboration','possess',\n",
    "                    'environment', 'success', 'successful', 'candidate', 'candidates','want',\n",
    "                    'requirements', 'required', 'preferred', 'preferably','opportunities','opportunity',\n",
    "                    'skillset', 'apply', 'apply now', 'apply online', 'apply today', 'apply here', 'apply button',\n",
    "                    'company','companies','companys', 'organization', 'industry', 'sector', 'field', 'domain', 'working',\n",
    "                    'teamwork', 'team player', 'employee', 'employees', 'colleague', 'colleagues', 'professional',\n",
    "                    'professionals', 'individual', 'individuals', 'managers','scientist','integrity',\n",
    "                    'direct', 'supervisor','regional','physical','mental','disabilities',\n",
    "                    'supervisory', 'managing', 'managed', 'manageable', 'performance', 'perform',\n",
    "                    'performing', 'performed', 'goal', 'goals', 'objective', 'objectives', 'outcome', 'outcomes',\n",
    "                    'initiative', 'initiatives', 'innovate','view','help','different',\n",
    "                    'innovates', 'innovated', 'innovating', 'solution', 'solutions', 'creativity',\n",
    "                    'create', 'creates', 'created', 'creating', 'results', 'outcome', 'outcomes','looking',\n",
    "                    'implement', 'implements', 'implemented', 'implementing', 'develops', 'developed',\n",
    "                    'developing', 'designed', 'designing','level','needs','need','familiarity',\n",
    "                    'evaluated', 'evaluating', 'strong', 'excellent','committed','potential','employment',\n",
    "                    'effective', 'efficient', 'successful', 'outstanding', 'superior', 'proven', 'demonstrated',\n",
    "                    'abilities', 'aptitude', 'talent', 'talented','veteran','status','compensation','permanent'\n",
    "                    'experienced', 'expert', 'proficient', 'qualification', 'qualifications', 'degree', 'education',\n",
    "                    'required', 'preferred','based','intensive','hidden','presence','harassment','including',\n",
    "                    'industry', 'sector', 'field', 'domain', 'area', 'technical', 'technological','help',\n",
    "                    'technology', 'solution', 'solutions','person','right','passport','citizen','written',\n",
    "                    'think','existing','salary','consideration','miss','chance','vacancy','related','relevant',\n",
    "                    'procedure', 'method', 'best practices', 'standard', 'standards', 'compliance','changes',\n",
    "                    'regulation', 'regulatory', 'policy', 'policies', 'procedure', 'procedures', 'guideline',\n",
    "                    'guidelines', 'protocol', 'protocols', 'manual', 'manuals', 'document', 'documents',\n",
    "                    'collaborate', 'collaboration', 'coordinate', 'coordination','referred','following',\n",
    "                          'interact', 'interaction', 'interpersonal', 'teamwork', 'team player', 'cross-functional',\n",
    "                          'multi-disciplinary', 'multi-functional', 'work well', 'adapt', 'flexible', 'fast-paced',\n",
    "                          'innovation', 'creativity', 'solution-oriented','grasp','demand','accept','privacy','notice',\n",
    "                          'mentoring', 'coach','national','origin','based','gender','lunch','food','minimum',\n",
    "                          'coaching', 'train', 'training', 'develop', 'development', 'growth', 'learning', 'learner',\n",
    "                          'continuous improvement', 'professional development', 'career growth', 'self-starter', 'initiative',\n",
    "                          'self-motivated', 'motivation', 'enthusiastic', 'passionate','learn','willing','state','art']\n",
    "#print(other_stop_words[-1])\n",
    "# Create a new spaCy model with updated stop words\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#print(nlp('project')[0].is_stop)\n",
    "# Update stop words\n",
    "nlp.Defaults.stop_words |= set(other_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "905e1bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this functions returns a list of words after filtering out irrelevant words from the phrases\n",
    "import re\n",
    "abbrev_skills = [a.upper() for a in all_abbreviations]\n",
    "def key_words(phrase):\n",
    "    \n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9+#]+', ' ', str(phrase))\n",
    "    doc = nlp(cleaned_text)\n",
    "    \n",
    "    taggers = [\"learning\", \"programming\",\"tool\",'packages','tools','interface','service','studio',\n",
    "              'reality','365','networks','science','series']\n",
    "\n",
    "    key_words = []\n",
    "    i=0\n",
    "    while i< len(doc):\n",
    "    \n",
    "        token=doc[i]\n",
    "        if token.is_stop:\n",
    "            i+=1\n",
    "            continue\n",
    "        if token.pos_ in [\"PROPN\"] or token.text.upper() in abbrev_skills:\n",
    "            key_words.append(token.text)\n",
    "            i+=1\n",
    "            continue\n",
    "    \n",
    "        elif token.pos_ in [\"NOUN\",\"X\",\"ADV\"]:\n",
    "            combined_noun = token.text\n",
    "        \n",
    "            if i + 1 < len(doc) and doc[i + 1].text.lower() in  taggers:\n",
    "                combined_noun += \" \" + doc[i + 1].text\n",
    "                i += 1\n",
    "            key_words.append(combined_noun)\n",
    "        elif token.pos_ in [\"ADJ\",\"VERB\"]:\n",
    "        #print(token.text)\n",
    "        #converted_word = token.lemma_\n",
    "        # If you want to convert to a noun, use the .noun_ attribute\n",
    "        #if converted_word:\n",
    "            #combined_nouns.append(converted_word)\n",
    "        #else:\n",
    "            key_words.append(token.text)\n",
    "        i+=1\n",
    "    return key_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aec69ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#further cleaning the words where there is an issue with spaces.\n",
    "def keyword_map(text):\n",
    "    key_match = []\n",
    "    \n",
    "\n",
    "    for s in key_word_list:\n",
    "        if s in text:\n",
    "            key_match.append(s)\n",
    "    #print(len(key_match))\n",
    "    if len(key_match)==0:\n",
    "        #print(1)\n",
    "        key_match.append(text) \n",
    "\n",
    "    return key_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0b164f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract key words from the phrases generated from the model\n",
    "resume_keywords['key_words'] = resume_keywords['Phrase'].apply(lambda x: key_words(x))\n",
    "resume_keywords1 = resume_keywords[resume_keywords[\"key_words\"].apply(lambda x: len(x) > 0)]\n",
    "resume_keywords1 = resume_keywords1.explode('key_words', ignore_index=True)\n",
    "resume_keywords1['cleaned_keywords']=resume_keywords1['key_words'].apply(lambda x: keyword_map(x))\n",
    "resume_keywords1 = resume_keywords1.explode('cleaned_keywords', ignore_index=True)\n",
    "#resume_keywords.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a003cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Consultant' 'DevOps' 'business analyst' 'business manager'\n",
      " 'data analyst' 'data engineer' 'data scientist' 'database administrator'\n",
      " 'machine learning engineer' 'product manager' 'project manager'\n",
      " 'research scientist' 'software developer']\n"
     ]
    }
   ],
   "source": [
    "#get all the titles which were processed using the model using jod descriptions\n",
    "titles = result_df2['job_title'].unique()\n",
    "#print(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed6d7c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_keywords</th>\n",
       "      <th>Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>advanced</td>\n",
       "      <td>0.692085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>analysis</td>\n",
       "      <td>0.772277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>analytic</td>\n",
       "      <td>0.837631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anomalies</td>\n",
       "      <td>0.633634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer</td>\n",
       "      <td>0.645638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>assisting</td>\n",
       "      <td>0.408432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>assists</td>\n",
       "      <td>0.719214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>assurance</td>\n",
       "      <td>0.865631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>attacks</td>\n",
       "      <td>0.563710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>attacks tools</td>\n",
       "      <td>0.475892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cleaned_keywords    Scores\n",
       "0         advanced  0.692085\n",
       "1         analysis  0.772277\n",
       "2         analytic  0.837631\n",
       "3        anomalies  0.633634\n",
       "4           answer  0.645638\n",
       "5        assisting  0.408432\n",
       "6          assists  0.719214\n",
       "7        assurance  0.865631\n",
       "8          attacks  0.563710\n",
       "9    attacks tools  0.475892"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove duplicate key words and keep the max score for each\n",
    "resume_keywords2 = resume_keywords1.groupby(['cleaned_keywords'])['Scores'].max().reset_index(name='Scores')\n",
    "#resume_keywords2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec2e941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.12134\n",
      "92.93382\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job title</th>\n",
       "      <th>Match Score1</th>\n",
       "      <th>Match Score2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>data scientist</td>\n",
       "      <td>90.127777</td>\n",
       "      <td>56.231506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data engineer</td>\n",
       "      <td>79.476376</td>\n",
       "      <td>55.516540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data analyst</td>\n",
       "      <td>75.454283</td>\n",
       "      <td>56.707234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>machine learning engineer</td>\n",
       "      <td>72.215901</td>\n",
       "      <td>54.688661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>database administrator</td>\n",
       "      <td>70.209040</td>\n",
       "      <td>43.452612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business analyst</td>\n",
       "      <td>69.518616</td>\n",
       "      <td>53.299861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>product manager</td>\n",
       "      <td>69.356174</td>\n",
       "      <td>51.194810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DevOps</td>\n",
       "      <td>69.045185</td>\n",
       "      <td>49.606561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Consultant</td>\n",
       "      <td>68.388773</td>\n",
       "      <td>50.920409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business manager</td>\n",
       "      <td>64.359428</td>\n",
       "      <td>48.369488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>research scientist</td>\n",
       "      <td>63.937194</td>\n",
       "      <td>48.260707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>project manager</td>\n",
       "      <td>59.692783</td>\n",
       "      <td>48.206993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>software developer</td>\n",
       "      <td>56.920426</td>\n",
       "      <td>52.289781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Job title  Match Score1  Match Score2\n",
       "6              data scientist     90.127777     56.231506\n",
       "5               data engineer     79.476376     55.516540\n",
       "4                data analyst     75.454283     56.707234\n",
       "8   machine learning engineer     72.215901     54.688661\n",
       "7      database administrator     70.209040     43.452612\n",
       "2            business analyst     69.518616     53.299861\n",
       "9             product manager     69.356174     51.194810\n",
       "1                      DevOps     69.045185     49.606561\n",
       "0                  Consultant     68.388773     50.920409\n",
       "3            business manager     64.359428     48.369488\n",
       "11         research scientist     63.937194     48.260707\n",
       "10            project manager     59.692783     48.206993\n",
       "12         software developer     56.920426     52.289781"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#match the resume keywords with each job titles key words and take the scalar product of their score and normalize \n",
    "#the score on the scale of 0-1 or %scale\n",
    "\n",
    "s1 = set(resume_keywords2['cleaned_keywords'])\n",
    "#normalizing factor for 2 types of scoring\n",
    "total_score2 = resume_keywords2['Scores'].sum()\n",
    "total_score1 = (resume_keywords2['Scores']**2).sum()\n",
    "print(total_score1)\n",
    "print(total_score2)\n",
    "match_dict = {'Job title': [], 'Match Score1': [], 'Match Score2': []}\n",
    "best_match = ''\n",
    "best_score = 0\n",
    "for t in titles:\n",
    "    d2 = result_df2[result_df2[\"job_title\"]==t]\n",
    "        \n",
    "    s2 = set(d2['cleaned_keywords'])\n",
    "    skill_keys = s1.intersection(s2)\n",
    "    score1 = 0\n",
    "    score2 = 0\n",
    "    \n",
    "    #print(pair['Job1'],pair['Job2'])\n",
    "    if skill_keys:\n",
    "        for skill in skill_keys:\n",
    "                #resume score for skill\n",
    "            p1 = float(resume_keywords2[resume_keywords2['cleaned_keywords'] == skill]['Scores'].iloc[0])\n",
    "                #job desc score for skill\n",
    "            p2 = float(d2[d2['cleaned_keywords'] == skill]['avg_score'].iloc[0])\n",
    "            #print(float(p2))\n",
    "            #print(p1*p2)\n",
    "            #dot product for scoreing\n",
    "            score1 += p1*p2\n",
    "            #using only score from resume to calculate the job fit scoring\n",
    "            score2 += p1\n",
    "    score1 = score1*1000/total_score1\n",
    "    score2 = score2*100/total_score2\n",
    "    match_dict['Job title'].append(t)\n",
    "    match_dict['Match Score1'].append(score1)\n",
    "    match_dict['Match Score2'].append(score2)\n",
    "    if score1 > best_score:\n",
    "        best_score = score1\n",
    "        best_match = t\n",
    "            \n",
    "            \n",
    "    #if i<10:\n",
    "        #print(match_dict)\n",
    "        \n",
    "job_match = pd.DataFrame(match_dict)\n",
    "\n",
    "job_match = job_match.sort_values(by='Match Score1', ascending=False)\n",
    "\n",
    "#job_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "806d5807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Science\n"
     ]
    }
   ],
   "source": [
    "#print(resume_df['Category'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa60458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output that will be fed into the data visualization in real time\n",
    "job_match1 = job_match[['Job title','Match Score1']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
